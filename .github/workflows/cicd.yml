name: CI/CD for Airflow ETL DAG

on:
  push:
    branches:
      - main

jobs:
  build-test-deploy:
    runs-on: ubuntu-latest  # Uses your local runner to access Minikube

    steps:
      - name: Checkout code
        uses: actions/checkout@v4  # Downloads your repo code

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'  # Matches runner's Python version (log showed 3.12.11)

      - name: Install dependencies
        run: |
          pip install apache-airflow apache-airflow-providers-postgres pandas sqlalchemy psycopg2-binary pytest flake8  # Moved flake8 here

      - name: Lint code (Build/Check Syntax)
        run: |
          flake8 elt_dag.py  # Checks for Python style errors

      - name: Unit Test extract_and_load function
        run: |
          cat > test_elt.py << 'EOF'
          import pandas as pd
          import unittest
          from unittest.mock import patch, MagicMock
          import sys
          sys.path.insert(0, '.')  # Ensure current dir is in path for import
          from elt_dag import extract_and_load  # Import the function

          class TestExtractAndLoad(unittest.TestCase):
              def test_extract_and_load(self):
                  # Sample JSON data to mock (based on expected structure)
                  sample_json = {
                      "order_id": ["ORD001"],
                      "item_name": ["Widget A"],
                      "quantity": ["2"],
                      "price_per_unit": ["10.50"],
                      "total_price": ["$21.00"],
                      "order_date": ["2024-01-01"],
                      "region": ["North"],
                      "payment_method": ["Credit"],
                      "customer_info": [{
                          "customer_id": "CUST001",
                          "email": "a@example.com",
                          "age": "30",  # String to test dtype
                          "address": {
                              "street": "123 Main St",
                              "city": "Anytown",
                              "zip": "12345"
                          }
                      }],
                      "status": ["Shipped"],
                      "notes": ["Note1"]
                  }
                  
                  # Create expected flattened DataFrame for comparison (single row)
                  expected_data = {
                      'order_id': ['ORD001'],
                      'item_name': ['Widget A'],
                      'quantity': ['2'],
                      'price_per_unit': ['10.50'],
                      'total_price': ['$21.00'],
                      'order_date': ['2024-01-01'],
                      'region': ['North'],
                      'payment_method': ['Credit'],
                      'status': ['Shipped'],
                      'notes': ['Note1'],
                      'customer_info_customer_id': ['CUST001'],
                      'customer_info_email': ['a@example.com'],
                      'customer_info_age': ['30'],
                      'customer_info_address_street': ['123 Main St'],
                      'customer_info_address_city': ['Anytown'],
                      'customer_info_address_zip': ['12345']
                  }
                  expected_df = pd.DataFrame(expected_data)
                  
                  # Mock pd.read_json to return known df
                  mock_df = MagicMock()
                  mock_df.to_dict.return_value = pd.DataFrame(sample_json).to_dict('records')  # Correct format: list of dicts
                  mock_read_json = MagicMock(return_value=mock_df)
                  
                  # Mock PostgresHook instance and engine
                  mock_hook_instance = MagicMock()
                  mock_engine = MagicMock()
                  mock_conn = MagicMock()
                  mock_engine.connect.return_value.__enter__.return_value = mock_conn
                  mock_engine.connect.return_value.__exit__.return_value = False
                  mock_hook_instance.get_sqlalchemy_engine.return_value = mock_engine
                  
                  with patch('elt_dag.pd.read_json', mock_read_json), \
                       patch('elt_dag.os.path.join', return_value='/fake/path'), \
                       patch('elt_dag.PostgresHook', return_value=mock_hook_instance) as mock_postgres_class, \
                       patch('elt_dag.pd.json_normalize') as mock_normalize, \
                       patch('pandas.DataFrame.to_sql') as mock_to_sql:
                      
                      # Set up mock_normalize to return expected_df
                      mock_normalize.return_value = expected_df
                      
                      extract_and_load()
                  
                  # Assertions on class mock (constructor call)
                  mock_postgres_class.assert_called_once_with(postgres_conn_id='postgres_conn')
                  
                  # Assertions on instance methods
                  mock_hook_instance.get_sqlalchemy_engine.assert_called_once()
                  mock_engine.connect.assert_called_once()
                  
                  # Other assertions
                  mock_read_json.assert_called_once()
                  mock_normalize.assert_called_once()
                  mock_to_sql.assert_called_once()
                  
                  # Verify to_sql args
                  to_sql_args, to_sql_kwargs = mock_to_sql.call_args
                  df_flat = to_sql_args[0]  # The flattened df passed to to_sql
                  self.assertEqual(to_sql_kwargs['name'], 'staging_sales')
                  self.assertEqual(to_sql_kwargs['if_exists'], 'replace')
                  self.assertFalse(to_sql_kwargs['index'])
                  self.assertIn('customer_info_age', to_sql_kwargs['dtype'])
                  self.assertEqual(to_sql_kwargs['dtype']['customer_info_age'], 'TEXT')
                  
                  # Verify df_flat (flattening and column renaming)
                  expected_columns = [
                      'order_id', 'item_name', 'quantity', 'price_per_unit', 'total_price',
                      'order_date', 'region', 'payment_method', 'status', 'notes',
                      'customer_info_customer_id', 'customer_info_email', 'customer_info_age',
                      'customer_info_address_street', 'customer_info_address_city', 'customer_info_address_zip'
                  ]
                  self.assertListEqual(list(df_flat.columns), expected_columns)
                  self.assertEqual(len(df_flat), 1)  # From sample
                  self.assertEqual(df_flat['customer_info_age'].dtype, 'object')  # Text/string before SQL cast
                  self.assertEqual(df_flat['order_id'].iloc[0], 'ORD001')  # Sample value check
                  
                  # Compare the actual flattened DF to expected
                  pd.testing.assert_frame_equal(df_flat.reset_index(drop=True), expected_df.reset_index(drop=True))

          if __name__ == '__main__':
              unittest.main(verbosity=2, exit=False)
          EOF
          pytest test_elt.py -v  # Run with pytest for better output; uses unittest.TestCase
          rm -f test_elt.py  # Clean up test file

      - name: Deploy DAG to Airflow on Minikube
        run: |
          # Copy DAG and JSON to Airflow pod
          POD_NAME=$(kubectl get pods -n elt-pipeline -l component=scheduler -o jsonpath='{.items[0].metadata.name}')
          kubectl cp elt_dag.py airflow/$POD_NAME:/opt/airflow/dags/elt_dag.py -n elt-pipeline
          kubectl cp sales_record.json airflow/$POD_NAME:/opt/airflow/dags/sales_record.json -n elt-pipeline
          # Restart scheduler to pick up new DAG
          kubectl rollout restart deployment/airflow-scheduler -n elt-pipeline

      - name: Test DAG in Airflow
        run: |
          # Use Airflow CLI to test (install airflow if not)
          airflow dags test elt_sales_pipeline $(date +%Y-%m-%d)  # Tests without running tasks
