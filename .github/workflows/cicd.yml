name: CI/CD for Airflow ETL DAG

on:
  push:
    branches:
      - main

jobs:
  build-test-deploy:
    runs-on: ubuntu-latest  # Uses your local runner to access Minikube

    steps:
      - name: Checkout code
        uses: actions/checkout@v4  # Downloads your repo code

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'  # Matches runner's Python version (log showed 3.12.11)

      - name: Install dependencies
        run: |
          pip install apache-airflow apache-airflow-providers-postgres pandas sqlalchemy psycopg2-binary pytest  # Added pytest here to avoid reinstall

      - name: Lint code (Build/Check Syntax)
        run: |
          pip install flake8
          flake8 elt_dag.py  # Checks for Python style errors

      - name: Unit Test extract_and_load function
        run: |
          cat > test_elt.py << 'EOF'
          import pandas as pd
          import os
          from unittest.mock import patch, MagicMock
          import sys
          sys.path.insert(0, '.')  # Ensure current dir is in path for import
          from elt_dag import extract_and_load  # Import the function

          def test_extract_and_load():
              # Mock the PostgresHook and SQLAlchemy engine to avoid real DB connection
              mock_hook = MagicMock()
              mock_engine = MagicMock()
              mock_engine.connect.return_value.__exit__.return_value = None  # Mock context manager
              mock_engine.connect.return_value.execute.return_value = None  # Mock execute if needed
              
              with patch('elt_dag.PostgresHook') as mock_pg_hook:
                  mock_pg_hook.return_value.get_sqlalchemy_engine.return_value = mock_engine
                  extract_and_load()
              
              # Verify extraction happened (check that to_sql was called with expected data)
              mock_engine.connect.assert_called_once()
              pd_mock = mock_engine.connect.return_value.execute.call_args[0][0]  # This is approximate; adjust if needed
              
              # Alternative: Verify by mocking pd.read_json and checking df_flat
              # But since we mock the hook, we can also patch os.path to ensure file load
              # For simplicity, assert the mock was called (confirms function ran without error)
              mock_pg_hook.assert_called_once_with(postgres_conn_id='postgres_conn')
              
              # To verify data: We'd need to capture the df, but for now, test that function completes
              # (In a full test, you'd patch pd.read_json to return known data and assert on to_sql args)
          EOF
          pytest test_elt.py --verbose  # Now mocks DB, so no connection needed
          rm -f test_elt.py  # Clean up test file

      - name: Deploy DAG to Airflow on Minikube
        run: |
          # Copy DAG and JSON to Airflow pod
          POD_NAME=$(kubectl get pods -n elt-pipeline -l component=scheduler -o jsonpath='{.items[0].metadata.name}')
          kubectl cp elt_dag.py airflow/$POD_NAME:/opt/airflow/dags/elt_dag.py -n elt-pipeline
          kubectl cp sales_record.json airflow/$POD_NAME:/opt/airflow/dags/sales_record.json -n elt-pipeline
          # Restart scheduler to pick up new DAG
          kubectl rollout restart deployment/airflow-scheduler -n elt-pipeline

      - name: Test DAG in Airflow
        run: |
          # Use Airflow CLI to test (install airflow if not)
          airflow dags test elt_sales_pipeline $(date +%Y-%m-%d)  # Tests without running tasks
