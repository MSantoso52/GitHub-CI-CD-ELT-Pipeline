name: CI/CD for Airflow ETL DAG

on:
  push:
    branches:
      - main

jobs:
  build-test-deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          pip install apache-airflow apache-airflow-providers-postgres pandas sqlalchemy psycopg2-binary pytest flake8

      - name: Lint code (Build/Check Syntax)
        run: |
          flake8 elt_dag.py

      - name: Unit Test extract_and_load function
        run: |
          cat > test_elt.py << 'EOF'
          import pandas as pd
          import unittest
          from unittest.mock import patch, MagicMock
          from sqlalchemy.types import Text
          import sys
          sys.path.insert(0, '.')
          from elt_dag import extract_and_load

          class TestExtractAndLoad(unittest.TestCase):
            def test_extract_and_load(self):
              sample_data = [{
                "order_id": "ORD001",
                "item_name": "Widget A",
                "quantity": "2",
                "price_per_unit": "10.50",
                "total_price": "$21.00",
                "order_date": "2024-01-01",
                "region": "North",
                "payment_method": "Credit",
                "customer_info": {
                  "customer_id": "CUST001",
                  "email": "a@example.com",
                  "age": "30",
                  "address": {
                    "street": "123 Main St",
                    "city": "Anytown",
                    "zip": "12345"
                  }
                },
                "status": "Shipped",
                "notes": "Note1"
              }]
              sample_df = pd.DataFrame(sample_data)
              mock_read_json = MagicMock(return_value=sample_df)
              expected_data = {
                'order_id': ['ORD001'],
                'item_name': ['Widget A'],
                'quantity': ['2'],
                'price_per_unit': ['10.50'],
                'total_price': ['$21.00'],
                'order_date': ['2024-01-01'],
                'region': ['North'],
                'payment_method': ['Credit'],
                'status': ['Shipped'],
                'notes': ['Note1'],
                'customer_info_customer_id': ['CUST001'],
                'customer_info_email': ['a@example.com'],
                'customer_info_age': ['30'],
                'customer_info_address_street': ['123 Main St'],
                'customer_info_address_city': ['Anytown'],
                'customer_info_address_zip': ['12345']
              }
              expected_df = pd.DataFrame(expected_data)
              mock_hook_instance = MagicMock()
              mock_engine = MagicMock()
              mock_hook_instance.get_sqlalchemy_engine.return_value = mock_engine
              with patch('elt_dag.pd.read_json', mock_read_json), \
                   patch('elt_dag.os.path.join', return_value='/fake/path'), \
                   patch('elt_dag.PostgresHook', return_value=mock_hook_instance) as mock_postgres_class, \
                   patch('elt_dag.pd.json_normalize') as mock_normalize, \
                   patch('pandas.DataFrame.to_sql') as mock_to_sql:
                mock_normalize.return_value = expected_df
                extract_and_load()
              mock_postgres_class.assert_called_once_with(postgres_conn_id='postgres_conn')
              mock_hook_instance.get_sqlalchemy_engine.assert_called_once()
              mock_read_json.assert_called_once_with('/fake/path')
              mock_normalize.assert_called_once_with(sample_df.to_dict('records'))
              mock_to_sql.assert_called_once()
              to_sql_args, to_sql_kwargs = mock_to_sql.call_args
              df_flat, con, *_ = to_sql_args
              self.assertIs(con, mock_engine)
              self.assertEqual(to_sql_kwargs['name'], 'staging_sales')
              self.assertEqual(to_sql_kwargs['if_exists'], 'replace')
              self.assertFalse(to_sql_kwargs['index'])
              self.assertIn('customer_info_age', to_sql_kwargs['dtype'])
              self.assertIsInstance(to_sql_kwargs['dtype']['customer_info_age'], Text)
              expected_columns = [
                'order_id', 'item_name', 'quantity', 'price_per_unit', 'total_price',
                'order_date', 'region', 'payment_method', 'status', 'notes',
                'customer_info_customer_id', 'customer_info_email', 'customer_info_age',
                'customer_info_address_street', 'customer_info_address_city', 'customer_info_address_zip'
              ]
              self.assertListEqual(list(df_flat.columns), expected_columns)
              self.assertEqual(len(df_flat), 1)
              self.assertEqual(df_flat['customer_info_age'].dtype, 'object')
              self.assertEqual(df_flat['order_id'].iloc[0], 'ORD001')
              pd.testing.assert_frame_equal(df_flat.reset_index(drop=True), expected_df.reset_index(drop=True))
          if __name__ == '__main__':
            unittest.main(verbosity=2, exit=False)
          EOF
          python test_elt.py

      - name: Start Minikube (if not running)
        run: |
          minikube version || { echo "Minikube not installed"; exit 1; }
          minikube status || echo "Minikube status check failed"
          if ! minikube status | grep -q "host: Running"; then
            echo "Starting Minikube..."
            minikube start --driver=docker --wait=all --wait-timeout=600s --memory=4096 --cpus=2 || { echo "Minikube start failed"; exit 1; }
          else
            echo "Minikube already running."
          fi
          kubectl cluster-info || { echo "Cluster info failed"; exit 1; }
          kubectl get nodes -o wide || { echo "Failed to get nodes"; exit 1; }
          kubectl wait --for=condition=Ready pod --all -n kube-system --timeout=300s || { echo "Cluster not ready"; exit 1; }
          kubectl config use-context minikube || { echo "Failed to set context"; exit 1; }

      - name: Verify Namespace Creation
        run: |
          kubectl create namespace elt-pipeline --dry-run=client -o yaml | kubectl apply -f - || { echo "Failed to create namespace"; exit 1; }
          kubectl get namespace elt-pipeline -o yaml || { echo "Failed to verify namespace"; exit 1; }

      - name: Verify Repository Files
        run: |
          ls -l elt_dag.py sales_record.json || { echo "Error: Required files missing"; exit 1; }

      - name: Verify Helm values.yaml
        run: |
          ls -l values.yaml || { echo "Error: values.yaml not found"; exit 1; }
          cat values.yaml

      - name: Install Airflow in elt-pipeline Namespace
        run: |
          helm repo add apache-airflow https://airflow.apache.org
          helm repo update
          helm search repo apache-airflow/airflow --versions
          kubectl create namespace elt-pipeline --dry-run=client -o yaml | kubectl apply -f -
          helm upgrade --install airflow apache-airflow/airflow \
            --version 1.15.0 \
            --namespace elt-pipeline \
            --values values.yaml \
            --set postgresql.enabled=true \
            --set dags.gitSync.enabled=true \
            --debug || { echo "Helm install failed"; exit 1; }
          kubectl wait --for=condition=Ready pod -l app.kubernetes.io/component=scheduler -n elt-pipeline --timeout=900s || { echo "Scheduler pod not ready"; exit 1; }
          kubectl get pods -n elt-pipeline --show-labels

      - name: Debug Scheduler Pod
        if: failure()
        run: |
          POD_NAME=$(kubectl get pods -n elt-pipeline -l app.kubernetes.io/component=scheduler -o jsonpath='{.items[0].metadata.name}' || echo "")
          if [ -n "$POD_NAME" ]; then
            echo "Scheduler pod logs:"
            kubectl logs -n elt-pipeline $POD_NAME
            echo "Scheduler pod description:"
            kubectl describe pod -n elt-pipeline $POD_NAME
          else
            echo "No scheduler pod found"
            kubectl get pods -n elt-pipeline --show-labels
          fi

      - name: Deploy DAG to Airflow on Minikube
        run: |
          POD_NAME=$(kubectl get pods -n elt-pipeline -l app.kubernetes.io/component=scheduler -o jsonpath='{.items[0].metadata.name}' || echo "")
          if [ -z "$POD_NAME" ]; then
            echo "Error: No scheduler pod found in namespace elt-pipeline"
            kubectl get pods -n elt-pipeline --show-labels
            exit 1
          fi
          echo "Scheduler pod: $POD_NAME"
          ls -l elt_dag.py sales_record.json || { echo "Files not found"; exit 1; }
          kubectl cp elt_dag.py airflow/$POD_NAME:/opt/airflow/dags/elt_sales_pipeline.py -n elt-pipeline
          kubectl cp sales_record.json airflow/$POD_NAME:/opt/airflow/dags/sales_record.json -n elt-pipeline
          kubectl rollout restart deployment/airflow-scheduler -n elt-pipeline || { echo "Rollout restart failed"; exit 1; }
          kubectl rollout status deployment/airflow-scheduler -n elt-pipeline --timeout=120s

      - name: Test DAG in Airflow
        run: |
          POD_NAME=$(kubectl get pods -n elt-pipeline -l app.kubernetes.io/component=scheduler -o jsonpath='{.items[0].metadata.name}' || echo "")
          if [ -z "$POD_NAME" ]; then
            echo "Error: No scheduler pod found"
            kubectl get pods -n elt-pipeline --show-labels
            exit 1
          fi
          kubectl exec -n elt-pipeline $POD_NAME -- airflow version || { echo "Airflow CLI not available"; exit 1; }
          kubectl exec -n elt-pipeline $POD_NAME -- airflow dags test elt_sales_pipeline $(date +%Y-%m-%d) --timeout=300s || { echo "DAG test failed"; exit 1; }
