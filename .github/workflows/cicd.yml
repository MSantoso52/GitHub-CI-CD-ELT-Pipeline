name: CI/CD for Airflow ETL DAG

on:
  push:
    branches:
      - main

jobs:
  build-test-deploy:
    runs-on: ubuntu-latest  # Self-hosted runner with Minikube access; ensure minikube/kubectl/helm installed

    steps:
      - name: Checkout code
        uses: actions/checkout@v4  # Downloads your repo code

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'  # Matches runner's Python version

      - name: Install dependencies
        run: |
          pip install apache-airflow apache-airflow-providers-postgres pandas sqlalchemy psycopg2-binary pytest flake8

      - name: Lint code (Build/Check Syntax)
        run: |
          flake8 elt_dag.py  # Fixed filename to match actual file

      - name: Unit Test extract_and_load function
        run: |
          cat > test_elt.py << 'EOF'
          import pandas as pd
          import unittest
          from unittest.mock import patch, MagicMock
          from sqlalchemy.types import Text
          import sys
          sys.path.insert(0, '.')  # Ensure current dir is in path for import
          from elt_dag import extract_and_load  # Matches actual filename

          class TestExtractAndLoad(unittest.TestCase):
            def test_extract_and_load(self):
              # Sample JSON data to mock (based on expected structure; using list of dicts for read_json)
              sample_data = [{
                "order_id": "ORD001",
                "item_name": "Widget A",
                "quantity": "2",
                "price_per_unit": "10.50",
                "total_price": "$21.00",
                "order_date": "2024-01-01",
                "region": "North",
                "payment_method": "Credit",
                "customer_info": {
                  "customer_id": "CUST001",
                  "email": "a@example.com",
                  "age": "30",  # String to test dtype
                  "address": {
                    "street": "123 Main St",
                    "city": "Anytown",
                    "zip": "12345"
                  }
                },
                "status": "Shipped",
                "notes": "Note1"
              }]
      
              # Mock pd.read_json to return known df (real DataFrame, not MagicMock, to avoid to_dict issues)
              sample_df = pd.DataFrame(sample_data)
              mock_read_json = MagicMock(return_value=sample_df)
      
              # Expected flattened DataFrame after json_normalize and column renaming
              expected_data = {
                'order_id': ['ORD001'],
                'item_name': ['Widget A'],
                'quantity': ['2'],
                'price_per_unit': ['10.50'],
                'total_price': ['$21.00'],
                'order_date': ['2024-01-01'],
                'region': ['North'],
                'payment_method': ['Credit'],
                'status': ['Shipped'],
                'notes': ['Note1'],
                'customer_info_customer_id': ['CUST001'],
                'customer_info_email': ['a@example.com'],
                'customer_info_age': ['30'],
                'customer_info_address_street': ['123 Main St'],
                'customer_info_address_city': ['Anytown'],
                'customer_info_address_zip': ['12345']
              }
              expected_df = pd.DataFrame(expected_data)
      
              # Mock PostgresHook instance and engine (no connect context manager needed)
              mock_hook_instance = MagicMock()
              mock_engine = MagicMock()
              mock_hook_instance.get_sqlalchemy_engine.return_value = mock_engine
      
              with patch('elt_dag.pd.read_json', mock_read_json), \
                   patch('elt_dag.os.path.join', return_value='/fake/path'), \
                   patch('elt_dag.PostgresHook', return_value=mock_hook_instance) as mock_postgres_class, \
                   patch('elt_dag.pd.json_normalize') as mock_normalize, \
                   patch('pandas.DataFrame.to_sql') as mock_to_sql:
              
                # Set up mock_normalize to return expected_df (after flattening/renaming simulation)
                mock_normalize.return_value = expected_df
                
                extract_and_load()
      
              # Assertions on class mock (constructor call)
              mock_postgres_class.assert_called_once_with(postgres_conn_id='postgres_conn')
      
              # Assertions on instance methods
              mock_hook_instance.get_sqlalchemy_engine.assert_called_once()
      
              # Other assertions
              mock_read_json.assert_called_once_with('/fake/path')
              mock_normalize.assert_called_once_with(sample_df.to_dict('records'))
              mock_to_sql.assert_called_once()
      
              # Verify to_sql args (passed df_flat, engine as con, and kwargs)
              to_sql_args, to_sql_kwargs = mock_to_sql.call_args
              df_flat, con, *_ = to_sql_args  # First arg: df, second: con (engine)
              self.assertIs(con, mock_engine)  # Verify engine is passed as con
              self.assertEqual(to_sql_kwargs['name'], 'staging_sales')
              self.assertEqual(to_sql_kwargs['if_exists'], 'replace')
              self.assertFalse(to_sql_kwargs['index'])
              self.assertIn('customer_info_age', to_sql_kwargs['dtype'])
              self.assertIsInstance(to_sql_kwargs['dtype']['customer_info_age'], Text)  # Check type, not string 'Text'
      
              # Verify df_flat (flattening and column renaming)
              expected_columns = [
                'order_id', 'item_name', 'quantity', 'price_per_unit', 'total_price',
                'order_date', 'region', 'payment_method', 'status', 'notes',
                'customer_info_customer_id', 'customer_info_email', 'customer_info_age',
                'customer_info_address_street', 'customer_info_address_city', 'customer_info_address_zip'
              ]
              self.assertListEqual(list(df_flat.columns), expected_columns)
              self.assertEqual(len(df_flat), 1)  # From sample
              self.assertEqual(df_flat['customer_info_age'].dtype, 'object')  # String before SQL cast
              self.assertEqual(df_flat['order_id'].iloc[0], 'ORD001')  # Sample value check
      
              # Compare the actual flattened DF to expected
              pd.testing.assert_frame_equal(df_flat.reset_index(drop=True), expected_df.reset_index(drop=True))

          if __name__ == '__main__':
            unittest.main(verbosity=2, exit=False)
          EOF
          python test_elt.py  # Run the generated test file

      - name: Start Minikube (if not running)
        run: |
          # Check if Minikube is running; start if not
          if ! minikube status | grep -q "host: Running"; then
            echo "Starting Minikube..."
            minikube start --driver=docker  # Use docker driver; adjust if needed (e.g., --driver=virtualbox)
          else
            echo "Minikube already running."
          fi
          # Wait for cluster to be ready
          kubectl wait --for=condition=Ready pod --all -n kube-system --timeout=300s
          # Update kubeconfig context to minikube
          kubectl config use-context minikube

      - name: Install Airflow in elt-pipeline Namespace
        run: |
          # Add Helm repo for Airflow (if not already added)
          helm repo add apache-airflow https://airflow.apache.org
          helm repo update
          # Create namespace if not exists
          kubectl create namespace elt-pipeline --dry-run=client -o yaml | kubectl apply -f -
          # Install/upgrade Airflow using Helm chart (use values.yaml if in repo; customize postgres_conn_id, etc.)
          helm upgrade --install airflow apache-airflow/airflow \
            --namespace elt-pipeline \
            --values values.yaml \  # Assumes values.yaml in repo; remove if using defaults and add --set executor=KubernetesExecutor, etc.
            --set postgresql.enabled=true \  # Enable PostgreSQL subchart
            --set dags.gitSync.enabled=true \  # Optional: For git-sync if needed; adjust as per your setup
          # Wait for scheduler pod to be ready (standard Helm label)
          kubectl wait --for=condition=Ready pod -l app.kubernetes.io/component=scheduler -n elt-pipeline --timeout=600s

      - name: Verify Airflow Namespace and Pod
        run: |
          # Wait for scheduler pod to be ready (use correct label selector for standard Airflow Helm deployment)
          kubectl wait --for=condition=Ready pod -l app.kubernetes.io/component=airflow-scheduler -n elt-pipeline --timeout=300s

      - name: Deploy DAG to Airflow on Minikube
        run: |
          # Copy DAG and JSON to Airflow pod (fixed filename and label)
          POD_NAME=$(kubectl get pods -n elt-pipeline -l app.kubernetes.io/component=scheduler -o jsonpath='{.items[0].metadata.name}')
          kubectl cp elt_dag.py airflow/$POD_NAME:/opt/airflow/dags/elt_sales_pipeline.py -n elt-pipeline  # Renamed to match DAG id
          kubectl cp sales_record.json airflow/$POD_NAME:/opt/airflow/dags/sales_record.json -n elt-pipeline  # Assumes sales_record.json is in repo
          # Restart scheduler to pick up new DAG
          kubectl rollout restart deployment/airflow-scheduler -n elt-pipeline
          # Wait for restart
          kubectl rollout status deployment/airflow-scheduler -n elt-pipeline --timeout=120s

      - name: Test DAG in Airflow
        run: |
          # Get pod name again after restart
          POD_NAME=$(kubectl get pods -n elt-pipeline -l app.kubernetes.io/component=scheduler -o jsonpath='{.items[0].metadata.name}')
          # Exec into pod and run airflow dags test (assumes airflow CLI is available in pod)
          kubectl exec -n elt-pipeline $POD_NAME -- airflow dags test elt_sales_pipeline $(date +%Y-%m-%d)
